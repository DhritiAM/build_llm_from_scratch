{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pHHjqAWDILdN"
      },
      "outputs": [],
      "source": [
        "# Import Regex library\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenizing\n",
        "text = \"Hello, there. How are you doing?\"\n",
        "result = re.split(r\"([,?.]|\\s)\", text)\n",
        "result = [item for item in result if item.strip()]\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ir13slcFLz1y",
        "outputId": "b78e23ad-3b14-4d6e-8e9d-37de7cd63ae8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello', ',', 'there', '.', 'How', 'are', 'you', 'doing', '?']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create vocabulary using the text file\n",
        "#Do we consider At and at with different Token IDs\n",
        "with open(\"/content/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "  raw_text = f.read()\n",
        "\n",
        "preprocessed = re.split(r\"([,.:;?\\\"'!_]|--|\\s)\", raw_text)\n",
        "preprocessed = [item for item in preprocessed if item.strip()]\n",
        "\n",
        "#Handle unknown end of text tokens\n",
        "preprocessed.append(\"<UNK>\")\n",
        "preprocessed.append(\"<EOT>\")\n",
        "\n",
        "vocab = {token: idx for idx, token in enumerate(set(preprocessed))}\n"
      ],
      "metadata": {
        "id": "07Ws__1mPJjZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementing Text tokenizer\n",
        "class textTokenizer:\n",
        "\n",
        "  def __init__(self, vocab):\n",
        "    self.str_to_int = vocab\n",
        "    self.int_to_str = {idx: token for token, idx in vocab.items()}\n",
        "\n",
        "  def encode(self, text):\n",
        "    preprocessed = re.split(r\"([,.:;?\\\"'!_]|--|\\s)\", text)\n",
        "    preprocessed = [item.strip() for item in preprocessed if item.strip()]\n",
        "    preprocessed = [token if token in self.str_to_int else \"<UNK>\" for token in preprocessed]\n",
        "    encoded = [self.str_to_int[token] for token in preprocessed]\n",
        "    return encoded\n",
        "\n",
        "\n",
        "  def decode(self, encoded):\n",
        "    # Modified to accept a list of integer IDs\n",
        "    decoded = \" \".join([self.int_to_str[idx] for idx in encoded])\n",
        "    decoded = re.sub(r\"\\s+([,.:;?\\\"'!_])\", r\"\\1\", decoded)\n",
        "    return decoded"
      ],
      "metadata": {
        "id": "LzzVCi9jdy7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "textTokenizer = textTokenizer(vocab)"
      ],
      "metadata": {
        "id": "4wuD6SmYrEPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded = textTokenizer.encode(\"It's the last you painted you know.\")\n",
        "print(encoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2nIBygUraUl",
        "outputId": "1274bf12-c735-4658-9a22-5d61eb1f527e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[247, 848, 1022, 708, 745, 654, 452, 654, 868, 544]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded = textTokenizer.decode(encoded)\n",
        "print(decoded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgzPZmuvscnM",
        "outputId": "c986800a-81fd-4129-c771-fc0c43ba92d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It' s the last you painted you know.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#With unknown tokens and end of text. Tokens not in vocabulary cannot be decoded.Hence lossy.\n",
        "encodedNew = textTokenizer.encode(\"Hello, there. How are you doing?\" + \"<EOT>\")\n",
        "decodedNew = textTokenizer.decode(encodedNew)\n",
        "decodedNew"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "IGTG4sLoSYZe",
        "outputId": "1f4c38f3-f388-4d18-da0f-6f99706b0baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<UNK>, there. How are you doing? <EOT>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Byte pair encoding - For efficient handling of unknown tokens, it is lossless\n",
        "import tiktoken\n",
        "\n",
        "tikTokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "encode = tikTokenizer.encode(\"Hey, It's a lovely morning. <|endoftext|> Wish you a very happy 16!\",\n",
        "                             allowed_special={\"<|endoftext|>\"})\n",
        "decode = tikTokenizer.decode(encode)\n",
        "\n",
        "for tokenID in encode:\n",
        "  subword = tikTokenizer.decode_single_token_bytes(tokenID).decode(\"utf-8\")\n",
        "  print(f\"Token ID: {tokenID}, subword: {subword}\")\n",
        "\n",
        "print(f\"Encoded Token IDs: {encode}\")\n",
        "print(f\"Decode from Token IDs: {decode}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84kXv6d8btEi",
        "outputId": "5e3576f1-5b00-49c0-9063-0b6e76ff0428"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token ID: 10814, subword: Hey\n",
            "Token ID: 11, subword: ,\n",
            "Token ID: 632, subword:  It\n",
            "Token ID: 338, subword: 's\n",
            "Token ID: 257, subword:  a\n",
            "Token ID: 14081, subword:  lovely\n",
            "Token ID: 3329, subword:  morning\n",
            "Token ID: 13, subword: .\n",
            "Token ID: 220, subword:  \n",
            "Token ID: 50256, subword: <|endoftext|>\n",
            "Token ID: 23447, subword:  Wish\n",
            "Token ID: 345, subword:  you\n",
            "Token ID: 257, subword:  a\n",
            "Token ID: 845, subword:  very\n",
            "Token ID: 3772, subword:  happy\n",
            "Token ID: 1467, subword:  16\n",
            "Token ID: 0, subword: !\n",
            "Encoded Token IDs: [10814, 11, 632, 338, 257, 14081, 3329, 13, 220, 50256, 23447, 345, 257, 845, 3772, 1467, 0]\n",
            "Decode from Token IDs: Hey, It's a lovely morning. <|endoftext|> Wish you a very happy 16!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data sampling with sliding window\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDataset(Dataset):\n",
        "  def __init__(self, text, tokenizer, maxLen, stride):\n",
        "    self.input_ids = []\n",
        "    self.target_ids = []\n",
        "    tokenIDs = tokenizer.encode(text)\n",
        "    for i in range(0, len(tokenIDs)-maxLen, stride):\n",
        "      self.input_ids.append(torch.tensor(tokenIDs[i:i+maxLen]))\n",
        "      self.target_ids.append(torch.tensor(tokenIDs[i+1:i+maxLen+1]))\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.input_ids)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "Wnkal-_xqzVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"In the heart of a bustling city, surrounded by towering glass buildings and the constant hum of traffic, a small bookstore sat quietly between a café and a flower shop. Inside, the scent of old pages mingled with fresh coffee from next door. People wandered in not just to buy books, but to lose themselves in worlds unknown, to find forgotten authors, or simply to enjoy the stillness that lingered despite the chaos outside. On rainy afternoons, the store felt like a sanctuary — a place where time paused, and stories whispered from dusty shelves.\"\n",
        "gptDataset = GPTDataset(text, tikTokenizer, 4, 1)"
      ],
      "metadata": {
        "id": "aXciKXZQ7Hb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gptDataset.__getitem__(2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LuqwR3St835T",
        "outputId": "918ba161-7045-4238-93ee-258d3dcb5285"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 2612,   286,   257, 46609]), tensor([  286,   257, 46609,  1748]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader(text, batch_size=4, maxLen=256, stride=128,\n",
        "                      shuffle=True, drop_last=True, num_workers=0):\n",
        "  tikTokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "  gptDataset = GPTDataset(text, tikTokenizer, maxLen, stride)\n",
        "  dataLoader = DataLoader(gptDataset,\n",
        "                          batch_size=batch_size,\n",
        "                          shuffle=shuffle,\n",
        "                          drop_last=drop_last,\n",
        "                          num_workers=num_workers)\n",
        "  return dataLoader"
      ],
      "metadata": {
        "id": "1tcYxH4w9AGE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = create_dataloader(raw_text, batch_size=2, maxLen=8, stride=2, shuffle=False)\n",
        "dataiter = iter(dataloader)\n",
        "first_batch = next(dataiter)\n",
        "second_batch = next(dataiter)\n",
        "inputs, target = next(dataiter)\n",
        "print(first_batch)\n",
        "print(second_batch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whPZHmHRdTjl",
        "outputId": "a5623b2a-e885-439c-92c2-12e1041bf8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[   40,   367,  2885,  1464,  1807,  3619,   402,   271],\n",
            "        [ 2885,  1464,  1807,  3619,   402,   271, 10899,  2138]]), tensor([[  367,  2885,  1464,  1807,  3619,   402,   271, 10899],\n",
            "        [ 1464,  1807,  3619,   402,   271, 10899,  2138,   257]])]\n",
            "[tensor([[ 1807,  3619,   402,   271, 10899,  2138,   257,  7026],\n",
            "        [  402,   271, 10899,  2138,   257,  7026, 15632,   438]]), tensor([[ 3619,   402,   271, 10899,  2138,   257,  7026, 15632],\n",
            "        [  271, 10899,  2138,   257,  7026, 15632,   438,  2016]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Word Embeddings\n",
        "vocab_size = 50357\n",
        "# output_dim = 256\n",
        "output_dim = 4\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "token_embeddings = token_embedding_layer(inputs)\n",
        "print(token_embeddings.shape)\n",
        "\n",
        "#Positional embedding\n",
        "context_length = 8\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "print(pos_embeddings.shape)\n",
        "\n",
        "#Final embedding\n",
        "final_embeddings = token_embeddings + pos_embeddings\n",
        "print(final_embeddings.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLMSh7HIwUWp",
        "outputId": "b9387fd0-60c8-4099-bf37-097db61c6ab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 8, 4])\n",
            "torch.Size([8, 4])\n",
            "torch.Size([2, 8, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Self-attention\n",
        "query = final_embeddings\n",
        "print(query[0][1]) # Considering batch1\n",
        "attn_scores_2 = torch.empty(query.shape[1])\n",
        "\n",
        "for i, x_i in enumerate(query[0]):\n",
        "  attn_scores_2[i] = torch.dot(x_i, query[0][1])\n",
        "\n",
        "print(attn_scores_2)\n",
        "\n",
        "#Apply Normalisation\n",
        "attn_score_2 = torch.softmax(attn_scores_2, dim=0)\n",
        "print(attn_score_2)\n",
        "print(attn_score_2.sum()) # Should be 1\n",
        "\n",
        "#Calculate context vector\n",
        "context_vec_2 = torch.zeros(query.shape[2])\n",
        "for i, x_i in enumerate(query[0]):\n",
        "  context_vec_2 += attn_score_2[i]*x_i\n",
        "\n",
        "print(context_vec_2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fmYT0vx_y2ms",
        "outputId": "504c7b09-ef70-48c6-e8b2-f02f9b40e529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.9279,  1.5110,  1.7080,  0.4483], grad_fn=<SelectBackward0>)\n",
            "tensor([ 1.3279,  6.2625, -0.7194,  3.2800,  3.6181, -0.6254, -0.1578, -0.0126],\n",
            "       grad_fn=<CopySlices>)\n",
            "tensor([6.3412e-03, 8.8155e-01, 8.1860e-04, 4.4664e-02, 6.2633e-02, 8.9925e-04,\n",
            "        1.4354e-03, 1.6596e-03], grad_fn=<SoftmaxBackward0>)\n",
            "tensor(1.0000, grad_fn=<SumBackward0>)\n",
            "tensor([-0.9938,  1.4182,  1.5692,  0.3468], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trial = torch.tensor(\n",
        "[[0.43, 0.15, 0.89], # Your (x^1)\n",
        "[0.55, 0.87, 0.66], # journey (x^2)\n",
        "[0.57, 0.85, 0.64], # starts (x^3)\n",
        "[0.22, 0.58, 0.33], # with (x^4)\n",
        "[0.77, 0.25, 0.10], # one (x^5)\n",
        "[0.05, 0.80, 0.55]] # step (x^6)\n",
        ")\n",
        "trial.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bjw3MXlf1eBg",
        "outputId": "019fe454-ab39-404a-b0c5-01a6766eb2d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "\n",
        "# 1. Tokenize\n",
        "text = \"Your journey begins with one step\"\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "tokens = tokenizer.encode(text)\n",
        "input_ids = torch.tensor(tokens)\n",
        "\n",
        "print(\"Token IDs:\", tokens)\n",
        "\n",
        "#Word Embeddings\n",
        "vocab_size = 50357\n",
        "output_dim = 3\n",
        "\n",
        "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
        "token_embeddings = token_embedding_layer(input_ids)\n",
        "print(token_embeddings)\n",
        "print(token_embeddings.shape)\n",
        "\n",
        "#Positional embedding\n",
        "context_length = 6\n",
        "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
        "pos_embeddings = pos_embedding_layer(torch.arange(context_length))\n",
        "print(pos_embeddings.shape)\n",
        "\n",
        "#Final embedding\n",
        "final_embeddings = token_embeddings + pos_embeddings\n",
        "print(final_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Om3RjmvLQNLJ",
        "outputId": "4e1d49ea-f81f-4b00-b4fd-f47b60aef794"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token IDs: [7120, 7002, 6140, 351, 530, 2239]\n",
            "tensor([[-0.9652,  0.3715,  1.7600],\n",
            "        [ 1.1019,  1.2620, -0.6656],\n",
            "        [ 0.4906,  2.1771,  0.2043],\n",
            "        [-0.3872,  0.8350, -0.9204],\n",
            "        [-0.7298, -2.5358,  1.4021],\n",
            "        [ 0.2725, -0.1636, -0.0523]], grad_fn=<EmbeddingBackward0>)\n",
            "torch.Size([6, 3])\n",
            "torch.Size([6, 3])\n",
            "tensor([[-0.9690,  0.0205,  1.4269],\n",
            "        [-0.8274,  2.2437,  0.4577],\n",
            "        [ 0.9628,  2.9165,  1.2324],\n",
            "        [ 1.3070,  0.5541, -1.4381],\n",
            "        [-2.0322, -1.9938,  1.8070],\n",
            "        [-1.5318, -1.8755, -0.3128]], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Self-Attention with trainable weights**"
      ],
      "metadata": {
        "id": "SYIYKuDG2nU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "nn.Linear is just a way of saying “I want to take each input vector and re-express it in a different space of size d_out.”"
      ],
      "metadata": {
        "id": "TUMx3s1gNQCL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "\n",
        "  def forward(self, x):\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    attn_scores = queries @ keys.T\n",
        "    attn_weights = torch.softmax(\n",
        "        attn_scores / keys.shape[-1]**0.5,\n",
        "        dim=-1\n",
        "    )\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec\n",
        "\n"
      ],
      "metadata": {
        "id": "ttbHFnEKveLa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention(3, 2)\n",
        "# print(sa_v2(final_embeddings))"
      ],
      "metadata": {
        "id": "ZZAG06ZlkZ4H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "queries = sa_v2.W_query(final_embeddings)\n",
        "keys = sa_v2.W_key(final_embeddings)\n",
        "values = sa_v2.W_value(final_embeddings)\n",
        "attn_scores = queries @ keys.T\n",
        "print(attn_scores)\n",
        "attn_weights = torch.softmax(\n",
        "    attn_scores / keys.shape[-1]**0.5,\n",
        "    dim=-1\n",
        ")\n",
        "context_vec = attn_weights @ values\n",
        "# print(context_vec)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN7i7j8JlV94",
        "outputId": "ce416dda-b523-4b62-ec8d-43ce8cec93a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.2148, -0.5406,  0.0258, -0.4695,  0.2444, -0.0820],\n",
            "        [ 2.8970,  1.2005, -0.0835, -1.2237,  1.8947,  2.6521],\n",
            "        [ 0.5015,  0.2924, -0.0181, -0.1070,  0.2559,  0.4377],\n",
            "        [ 2.5479,  1.3197, -0.0849, -0.7489,  1.4415,  2.2655],\n",
            "        [ 1.8434,  0.8863, -0.0585, -0.6267,  1.1012,  1.6565],\n",
            "        [ 2.4947,  1.0126, -0.0710, -1.0801,  1.6496,  2.2892]],\n",
            "       grad_fn=<MmBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Causal attention**\n",
        "\n",
        "Only the previous tokens influence the next token prediction, hence attn_scores of future tokens for a given query should be handled before applying softmax normalization"
      ],
      "metadata": {
        "id": "Yhk_6Q9BmTQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tril(attn_scores)\n",
        "\n",
        "# Create a boolean mask where elements are zero\n",
        "zero_mask = (x == 0)\n",
        "print(zero_mask)\n",
        "\n",
        "# Apply the mask to the matrix\n",
        "attn_scores = x.masked_fill_(zero_mask, -1*torch.inf)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwmv8DjQmsRa",
        "outputId": "12110177-5649-475d-fcbb-da6d21464f4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[False,  True,  True,  True,  True,  True],\n",
            "        [False, False,  True,  True,  True,  True],\n",
            "        [False, False, False,  True,  True,  True],\n",
            "        [False, False, False, False,  True,  True],\n",
            "        [False, False, False, False, False,  True],\n",
            "        [False, False, False, False, False, False]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attn_weights = torch.softmax(\n",
        "    attn_scores / keys.shape[-1]**0.5,\n",
        "    dim=-1\n",
        ")\n",
        "print(attn_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WBMjlcTOt2BX",
        "outputId": "7871b751-ae80-41e4-f955-e5ee22e4e1f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.7685, 0.2315, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3914, 0.3376, 0.2710, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5980, 0.2509, 0.0929, 0.0581, 0.0000, 0.0000],\n",
            "        [0.3945, 0.2005, 0.1028, 0.0688, 0.2334, 0.0000],\n",
            "        [0.3324, 0.1166, 0.0542, 0.0265, 0.1829, 0.2875]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Causal attention class handling multiple batches"
      ],
      "metadata": {
        "id": "D-7m46MmWUc8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, dropout, context_length, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "    self.dropout = nn.Dropout(dropout)\n",
        "    self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1)) # New\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, num_tokens, d_in = x.shape # New batch dimension b\n",
        "    keys = self.W_key(x)\n",
        "    queries = self.W_query(x)\n",
        "    values = self.W_value(x)\n",
        "\n",
        "    attn_scores = queries @ keys.transpose(1,2)\n",
        "    attn_scores.masked_fill_(  # New, _ ops are in-place\n",
        "            self.mask.bool()[:num_tokens, :num_tokens], -torch.inf)  # `:num_tokens` to account for cases where the number of tokens in the batch is smaller than the supported context_size\n",
        "    attn_weights = torch.softmax(\n",
        "            attn_scores / keys.shape[-1]**0.5, dim=-1\n",
        "        )\n",
        "    attn_weights = self.dropout(attn_weights) # New\n",
        "    context_vec = attn_weights @ values\n",
        "    return context_vec"
      ],
      "metadata": {
        "id": "2fmcH82CWbyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.triu(torch.ones(context_length, context_length), diagonal=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pd71IWqLjNVE",
        "outputId": "749d61c6-2d97-4ca8-efe0-3031bdef9bd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 1., 1., 1., 1., 1.],\n",
              "        [0., 0., 1., 1., 1., 1.],\n",
              "        [0., 0., 0., 1., 1., 1.],\n",
              "        [0., 0., 0., 0., 1., 1.],\n",
              "        [0., 0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 0., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "\n",
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhP0FaD_rvi6",
        "outputId": "ea3c8e82-4114-4b50-c01f-9206cfe4f7f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "d_in, d_out = 3, 2\n",
        "ca = CausalAttention(d_in, d_out, 0.0, context_length)\n",
        "context_vecs = ca(batch)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)\n",
        "print(context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x0taq3Ujr_1n",
        "outputId": "0f1ed11c-af8a-454b-a312-11dc271c6f60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context_vecs.shape: torch.Size([2, 6, 2])\n",
            "tensor([[[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]],\n",
            "\n",
            "        [[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self, d_in, d_out, num_heads, dropout, context_length, qkv_bias=False):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([CausalAttention(d_in, d_out, dropout, context_length, qkv_bias) for _ in range(num_heads)])\n",
        "\n",
        "  def forward(self, x):\n",
        "    return torch.cat([head(x) for head in self.heads], dim=-1)"
      ],
      "metadata": {
        "id": "B-aADcr896PN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "context_length = batch.shape[1]\n",
        "d_in, d_out = 3, 2\n",
        "num_heads = 2\n",
        "mha = MultiHeadAttention(d_in, d_out, num_heads, 0.0, context_length)\n",
        "context_vecs = mha(batch)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)\n",
        "print(context_vecs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "igkTVs_I-7yu",
        "outputId": "f8cf5883-356d-4f65-b187-df123fd8df7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "context_vecs.shape: torch.Size([2, 6, 4])\n",
            "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
            "\n",
            "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n"
          ]
        }
      ]
    }
  ]
}